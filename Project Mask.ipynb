{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from os import getcwd\n",
    "from shutil import copyfile\n",
    "from os import getcwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "950\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "main_dir = r'C:\\Users\\joaom\\OneDrive\\Desktop\\mask_or_nomask'\n",
    "mask_dir = r'C:\\Users\\joaom\\OneDrive\\Desktop\\mask_or_nomask\\maskd'\n",
    "no_mask_dir =r'C:\\Users\\joaom\\OneDrive\\Desktop\\mask_or_nomask\\nomaskd'\n",
    "test = r'C:\\Users\\joaom\\OneDrive\\Desktop\\mask_or_nomask\\test'\n",
    "train = r'C:\\Users\\joaom\\OneDrive\\Desktop\\mask_or_nomask\\train'\n",
    "train_mask = r'C:\\Users\\joaom\\OneDrive\\Desktop\\mask_or_nomask\\train\\train_mask'\n",
    "train_no_mask =r'C:\\Users\\joaom\\OneDrive\\Desktop\\mask_or_nomask\\train\\train_no_mask'\n",
    "test_mask = r'C:\\Users\\joaom\\OneDrive\\Desktop\\mask_or_nomask\\test\\test_mask'\n",
    "test_no_mask = r'C:\\Users\\joaom\\OneDrive\\Desktop\\mask_or_nomask\\test\\test_no_mask'\n",
    " \n",
    "print(len(os.listdir(main_dir)))\n",
    "print(len(os.listdir(mask_dir)))\n",
    "print(len(os.listdir(no_mask_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    source_file_list = random.sample(os.listdir(SOURCE), len(os.listdir(SOURCE)))\n",
    "    c=0\n",
    "    while c < len(source_file_list):\n",
    "        file = source_file_list[c]\n",
    "        if os.path.getsize(os.path.join(SOURCE,file)) > 0:\n",
    "            file_being_sent = os.path.join(SOURCE,file)\n",
    "            if len(os.listdir(TRAINING)) < len(source_file_list)*SPLIT_SIZE:\n",
    "                copyfile(file_being_sent,os.path.join(TRAINING,file))\n",
    "                b=1\n",
    "            else:\n",
    "                copyfile(file_being_sent,os.path.join(TESTING,file))\n",
    "        c+=1\n",
    "        \n",
    "split_size = .9\n",
    "split_data(mask_dir, train_mask, test_mask, split_size)\n",
    "split_data(no_mask_dir, train_no_mask, test_no_mask, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "855\n",
      "95\n",
      "900\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(train_mask)))\n",
    "print(len(os.listdir(test_mask)))\n",
    "print(len(os.listdir(train_no_mask)))\n",
    "print(len(os.listdir(test_no_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1755 images belonging to 2 classes.\n",
      "Found 195 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = r'C:\\Users\\joaom\\OneDrive\\Desktop\\mask_or_nomask\\train'\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# NOTE: YOU MUST USE A BATCH SIZE OF 10 (batch_size=10) FOR THE \n",
    "# TRAIN GENERATOR.\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
    "                                                    batch_size=10,\n",
    "                                                    class_mode='binary',\n",
    "                                                    target_size=(150, 150))\n",
    "\n",
    "VALIDATION_DIR = r'C:\\Users\\joaom\\OneDrive\\Desktop\\mask_or_nomask\\test'\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# NOTE: YOU MUST USE A BACTH SIZE OF 10 (batch_size=10) FOR THE \n",
    "# VALIDATION GENERATOR.\n",
    "validation_generator = train_datagen.flow_from_directory(VALIDATION_DIR,\n",
    "                                                    batch_size=10,\n",
    "                                                    class_mode='binary',\n",
    "                                                    target_size=(150, 150)) \n",
    "\n",
    "\n",
    "\n",
    "# Expected Output:\n",
    "# Found 2700 images belonging to 2 classes.\n",
    "# Found 300 images belonging to 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-7-445aef2f8ba7>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-445aef2f8ba7>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    epochs=1,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=1,\n",
    "                              verbose=1,\n",
    "                              validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask / .png\n",
      "Mask / .png\n",
      "Mask / _Mask.jpg\n",
      "Mask / .png\n",
      "Mask / _Mask.jpg\n",
      "Mask / .png\n",
      "Mask / _Mask.jpg\n",
      "Mask / .png\n",
      "Mask / _Mask.jpg\n",
      "Mask / .png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import google.colab\n",
    "# from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test = r'C:\\Users\\joaom\\OneDrive\\Desktop\\fjftyt\\01000'\n",
    "list1 = os.listdir(test)\n",
    "\n",
    "for value in list1[:10]:\n",
    "    img = image.load_img(os.path.join(test,value), target_size=(150, 150))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict(images, batch_size=10)\n",
    "    if str(classes) == '[[1.]]':\n",
    "        print('No Mask','/',str(value)[5:])\n",
    "    else : \n",
    "        print('Mask','/',str(value)[5:])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, image_np = cap.read()\n",
    "    img = cv2.resize(image_np,(150, 150))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    text = model.predict(img, batch_size=10)\n",
    "    if str(text) == '[[1.]]':\n",
    "        #print('No Mask','/',str(value)[5:])\n",
    "        r = 'No Mask'\n",
    "    else : \n",
    "        r= 'Mask'\n",
    "#         print('Mask','/',str(value)[5:])\n",
    "    \n",
    "    image_np = cv2.putText(image_np,f\"{r}, {text}\",(50, 100),cv2.FONT_HERSHEY_SIMPLEX, 1,(255, 255, 255),2) \n",
    "    cv2.imshow('mask detection',cv2.resize(image_np,(800,600)))\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
